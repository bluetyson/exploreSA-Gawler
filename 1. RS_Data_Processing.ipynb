{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  exploreSA-Gawler: Data Preparation for the SARIG Data Package\n",
    "\n",
    "  - Group: __TriPandas__\n",
    "  - Members: __Hugh Ouyang__, __Liang Chen__, __Wei Liu__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code set includes four files: \"1. RS_Data_Processing.ipynb\", \"2. Drillhole_Data_Processing.ipynb\", \"3. Field_Observation_Data_Processing.ipynb\" and \"4. MD_Data_Processing.ipynb\". These ipynb files are supposed to execute in the order suggested in the file names. \n",
    "\n",
    "It will automatically download the dataset from https://unearthed-exploresa.s3-ap-southeast-2.amazonaws.com/Unearthed_5_SARIG_Data_Package.zip and  save the precessed files to the directory under the name of the selected elements. \n",
    "\n",
    "A sample of the resulting dataset is shared at https://drive.google.com/drive/folders/18xBsRCAXP0cGsYl8L9PrtHkgy62cGRPL?usp=sharing\n",
    "\n",
    "If you have any question or comments, please comment it at the forum. We will update this code gradually as our data preparation and data engineering move on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RS data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n",
    "This section imports the required packages, download the dataset from the website and list the file names. As below, the data_loc is where the data set will be saved. To save storage space, the dataset won't be extracted as a folder. The data procesing will be done based via reading corresponding file contained in the zip file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:03:23.348679Z",
     "start_time": "2020-05-02T01:03:22.290507Z"
    }
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "pd.options.display.width=None\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "\n",
    "if sys.version_info >= (3, 6):\n",
    "    from zipfile import ZipFile as zipfile\n",
    "else:\n",
    "    import zipfile36 as zipfile\n",
    "    \n",
    "url = \"https://unearthed-exploresa.s3-ap-southeast-2.amazonaws.com/Unearthed_5_SARIG_Data_Package.zip\" \n",
    "# enter the directory to save data\n",
    "data_loc = 'D:/GitFolder/WorkBench/exploreSA-Gawler/data'\n",
    "file_name = 'Unearthed_5_SARIG_Data_Package.zip'\n",
    "\n",
    "if os.path.isfile(os.path.join(data_loc, file_name)):\n",
    "    print (\"File exists, No need to download.\")\n",
    "    pass\n",
    "else:\n",
    "    # open and save the zip file onto computer\n",
    "    url = urlopen(URL)\n",
    "    output = open('Unearthed_5_SARIG_Data_Package.zip', 'wb')    # note the flag:  \"wb\"        \n",
    "    output.write(url.read())\n",
    "    output.close()\n",
    "    \n",
    "files_in_dataset = []\n",
    "file_name = 'Unearthed_5_SARIG_Data_Package.zip'\n",
    "for file in zipfile(os.path.join(data_loc, file_name),'r').filelist:\n",
    "    files_in_dataset.append(file.filename)\n",
    "    \n",
    "files_in_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this part of data cleaning, we will only use the following files: \n",
    " - 'SARIG_Data_Package/sarig_rs_biostr_analys_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_biostr_results_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_chem_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_chem_isotope_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_details_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_geochron_ages_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_geochron_reslt_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_petrology_exp.csv',\n",
    " - 'SARIG_Data_Package/sarig_rs_reference_exp.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### record identifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:03:24.852660Z",
     "start_time": "2020-05-02T01:03:23.350674Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_reference_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_reference_exp.csv','r'), \n",
    "    sep=',', encoding='latin1')\n",
    "sarig_rs_reference_exp['SAMREF_CNO'] = sarig_rs_reference_exp['SAMREF_CNO'].astype('Int64')\n",
    "sarig_rs_reference_exp['PUBLICATION_DATE'] = pd.to_datetime(sarig_rs_reference_exp['PUBLICATION_DATE'])\n",
    "sarig_rs_reference_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:03:24.981344Z",
     "start_time": "2020-05-02T01:03:24.856649Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_reference_exp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAMPLE_NO and the SITE_NO both have no null and they have a multiple to one relationship, indicating that for one site,  there were one or more samples for experiments. The combination of SITE_NO and SAMPLE_NO can be the record identifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:03:24.999297Z",
     "start_time": "2020-05-02T01:03:24.983312Z"
    }
   },
   "outputs": [],
   "source": [
    "del sarig_rs_reference_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select chemical code of interest\n",
    "\n",
    "At first we need to standardize the unit in the dataset as the UNIT contains \"%\", \"ppb\" and other units.\n",
    "The column VALUE also contains different cases: numeric value, < number, > number. We cast <number as 0 and > number as number only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:03:25.011236Z",
     "start_time": "2020-05-02T01:03:25.002259Z"
    }
   },
   "outputs": [],
   "source": [
    "# define two functions to handle the cases of VALUE and UNIT\n",
    "def change_type(x):\n",
    "    '''handle three cases of VALUE, assigning it as 0 when it <d, and keep the value d when it >d'''\n",
    "    x = str(x)\n",
    "    if '<' in x:\n",
    "        return '0'\n",
    "    elif '>' in x:\n",
    "        return x.split('>')[1]\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "\n",
    "def change_units(x):\n",
    "    '''this function is to change the value and units. '''\n",
    "    if x[1] == \"%\":\n",
    "        x[0] = x[0]/10000\n",
    "        x[1] = 'ppm'\n",
    "    elif x[1] == 'ppb':\n",
    "        x[0] = x[0]*1000\n",
    "        x[1] = 'ppm'\n",
    "    elif x[1] == 'ug/l':\n",
    "        x[0] = x[0]*1000\n",
    "        x[1] = 'ppm'\n",
    "    else:\n",
    "        x[1] = 'ppm'\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:06:31.570107Z",
     "start_time": "2020-05-02T01:03:25.013232Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the sarig_rs_chem_exp by chunks to extract the set of CHEM_CODE\n",
    "sarig_rs_chem_exp_chunks = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_chem_exp.csv','r'), \n",
    "    sep=',', encoding='latin1', chunksize=100000)\n",
    "\n",
    "chem_code_set = set()\n",
    "for chunk in sarig_rs_chem_exp_chunks:\n",
    "#     chunk['COLLECTED_DATE'] = pd.to_datetime(chunk['COLLECTED_DATE'])\n",
    "#     chunk['VALUE'] = chunk['VALUE'].apply(lambda x: change_type(x))\n",
    "#     chunk['VALUE'] = chunk['VALUE'].astype('float32')\n",
    "#     chunk.loc[:, ['VALUE', 'UNIT']] = chunk.loc[:, ['VALUE', 'UNIT']].apply(lambda x: change_units(x), axis=1)\n",
    "    chem_code_set.update(set(chunk['CHEM_CODE'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:06:31.647375Z",
     "start_time": "2020-05-02T01:06:31.572075Z"
    }
   },
   "outputs": [],
   "source": [
    "# see some samples of the records\n",
    "chunk.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:06:31.676300Z",
     "start_time": "2020-05-02T01:06:31.652392Z"
    }
   },
   "outputs": [],
   "source": [
    "chem_elements = {}\n",
    "for item in sorted(list(chem_code_set)):\n",
    "    chem_elements[item] = [item]\n",
    "    \n",
    "chem_elements['Al'].extend(chem_elements['Al2O3'])\n",
    "chem_elements['Ba'].extend(chem_elements['BaO'])\n",
    "chem_elements['C'].extend(chem_elements['CO2'])\n",
    "chem_elements['C'].extend(chem_elements['CO3'])\n",
    "chem_elements['Ca'].extend(chem_elements['CaCO3'])\n",
    "chem_elements['Ca'].extend(chem_elements['CaO'])\n",
    "chem_elements['Ca'].extend(chem_elements['CaSO4'])\n",
    "chem_elements['Co'].extend(chem_elements['CoO'])\n",
    "chem_elements['Cr'].extend(chem_elements['Cr2O3'])\n",
    "chem_elements['Fe'].extend(chem_elements['Fe2'])\n",
    "chem_elements['Fe'].extend(chem_elements['Fe2O3'])\n",
    "chem_elements['Fe'].extend(chem_elements['FeO'])\n",
    "chem_elements['Fe'].extend(chem_elements['FeS2'])\n",
    "chem_elements['H2O'].extend(chem_elements['H2O_minus'])\n",
    "chem_elements['H2O'].extend(chem_elements['H2O_plus'])\n",
    "chem_elements['Mg'].extend(chem_elements['MgCO3'])\n",
    "chem_elements['Mg'].extend(chem_elements['MgO'])\n",
    "chem_elements['Mn'].extend(chem_elements['MnO'])\n",
    "chem_elements['Na'].extend(chem_elements['Na2O'])\n",
    "chem_elements['Na'].extend(chem_elements['NaCl'])\n",
    "chem_elements['Nb'].extend(chem_elements['Nb2O5'])\n",
    "chem_elements['Ni'].extend(chem_elements['NiO'])\n",
    "chem_elements['P'].extend(chem_elements['P2O5'])\n",
    "chem_elements['S'].extend(chem_elements['SO3'])\n",
    "chem_elements['S'].extend(chem_elements['SO4'])\n",
    "chem_elements['Si'].extend(chem_elements['SiO2'])\n",
    "chem_elements['Sr'].extend(chem_elements['Sr87_86'])\n",
    "chem_elements['Sr'].extend(chem_elements['SrO'])\n",
    "chem_elements['Ta'].extend(chem_elements['Ta2O5'])\n",
    "chem_elements['Th'].extend(chem_elements['ThO2'])\n",
    "chem_elements['Ti'].extend(chem_elements['TiO2'])\n",
    "chem_elements['U'].extend(chem_elements['U3O8'])\n",
    "chem_elements['V'].extend(chem_elements['V2O3'])\n",
    "chem_elements['V'].extend(chem_elements['V2O5'])\n",
    "chem_elements['W'].extend(chem_elements['WO3'])\n",
    "chem_elements['Zn'].extend(chem_elements['ZnO'])\n",
    "chem_elements['Zr'].extend(chem_elements['ZrO2'])\n",
    "\n",
    "modified = [ 'Al2O3', 'BaO', 'CO2','CO3',\n",
    " 'CaCO3',  'CaO', 'CaSO4', 'CoO', 'Cr2O3', 'Fe2', 'Fe2O3', 'Fe3', 'FeO', 'FeS2',\n",
    " 'H2O_minus', 'H2O_plus', 'K2O', 'MgCO3', 'MgO', 'MnO', 'NO3', 'Na2O', 'NaCl',\n",
    " 'Nb2O5', 'NiO', 'P2O5', 'SO3', 'SO4', 'SiO2', 'Sr87_86', 'SrO', 'Ta2O5', 'ThO2',\n",
    " 'TiO2', 'U3O8', 'V2O3', 'V2O5', 'WO3', 'ZnO', 'ZrO2', 'Total']\n",
    "for key in modified:\n",
    "    del chem_elements[key]\n",
    "    \n",
    "print(chem_elements.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using print(*sorted(list(chem_code_set)), sep = \", \") to print the set of the CHEM_CODE as \n",
    "\n",
    "[Ag, Al, Al2O3, As, Au, B, Ba, BaO, Be, Bi, Br, C, CO2, CO3, CPS_gamma, Ca, CaCO3, CaO, CaSO4, Cd, Ce, Cl, Co, CoO, Cr, Cr2O3, Cs, Cu, Dy, EC, Er, Eu, F, Fe, Fe2, Fe2O3, Fe3, FeO, FeS2, GPSM, Ga, Gd, Ge, GoI, H2O, H2O_minus, H2O_plus, HCO3, HMIN, Hf, Hg, Ho, I, In, Insol, Ir, K, K2O, LOI, La, Li, Lu, Mg, MgCO3, MgO, Mn, MnO, Mo, NO3, Na, Na2O, NaCl, Nb, Nb2O5, Nd, Ni, NiO, O18, Os, P, P2O5, Pb, Pd, Pr, Pt, RADBK, RADTC, Rb, Re, Rh, Ru, S, SO3, SO4, Sb, Sc, Se, Si, SiO2, Sm, Sn, Sr, Sr87_86, SrO, TOC, TOT/C, TOT/S, Ta, Ta2O5, Tb, Te, Th, ThO2, Ti, TiO2, Tl, Tm, Total, U, U3O8, V, V2O3, V2O5, W, WO3, Y, Yb, Zn, ZnO, Zr, ZrO2, pH]\n",
    "\n",
    "Oxides were converted and merged into their relevant individual elemental file. So do some other compounds. \n",
    "\n",
    "Then we can select the chemicals from the list below to extract data.\n",
    "['Ag', 'Al', 'As', 'Au', 'B', 'Ba', 'Be', 'Bi', 'Br', 'C', 'CPS_gamma', 'Ca', 'Cd', 'Ce', 'Cl', 'Co', 'Cr', 'Cs', 'Cu', 'Dy', 'EC', 'Er', 'Eu', 'F', 'Fe', 'GPSM', 'Ga', 'Gd', 'Ge', 'GoI', 'H2O', 'HCO3', 'HMIN', 'Hf', 'Hg', 'Ho', 'I', 'In', 'Insol', 'Ir', 'K', 'LOI', 'La', 'Li', 'Lu', 'Mg', 'Mn', 'Mo', 'Na', 'Nb', 'Nd', 'Ni', 'O18', 'Os', 'P', 'Pb', 'Pd', 'Pr', 'Pt', 'RADBK', 'RADTC', 'Rb', 'Re', 'Rh', 'Ru', 'S', 'Sb', 'Sc', 'Se', 'Si', 'Sm', 'Sn', 'Sr', 'TOC', 'TOT/C', 'TOT/S', 'Ta', 'Tb', 'Te', 'Th', 'Ti', 'Tl', 'Tm', 'U', 'V', 'W', 'Y', 'Yb', 'Zn', 'Zr', 'pH']\n",
    "\n",
    "Also, we are interested in some features only, so some features are ignored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T13:41:52.625909Z",
     "start_time": "2020-05-01T13:41:52.618902Z"
    }
   },
   "source": [
    "__Select the element from the dropdown list below for which you want to extract data. Then, \"Run All Below\" at the tab \"Cell\".__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T01:06:31.702256Z",
     "start_time": "2020-05-02T01:06:31.678293Z"
    }
   },
   "outputs": [],
   "source": [
    "dropdown_element = widgets.Dropdown(options= list(chem_elements.keys()))\n",
    "output_element = widgets.Output()\n",
    "def dropdown_element_eventhandler(change):\n",
    "    output_element.clear_output()\n",
    "    with output_element: \n",
    "        display(change.new)\n",
    "    \n",
    "\n",
    "dropdown_element.observe(dropdown_element_eventhandler, names='value')\n",
    "display(dropdown_element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:25:23.746619Z",
     "start_time": "2020-05-02T06:25:23.734681Z"
    }
   },
   "outputs": [],
   "source": [
    "element_selected = output_element.outputs[0]['data']['text/plain'].replace(\"'\", '')\n",
    "element_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:25:24.102787Z",
     "start_time": "2020-05-02T06:25:23.762605Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# select the interested chemical code\n",
    "\n",
    "interested_chemicals = chem_elements[element_selected]\n",
    "\n",
    "# columns carrrying useful info\n",
    "selected_chem_exp_cols = ['SAMPLE_NO', 'SAMPLE_SOURCE_CODE','ROCK_GROUP_CODE', \n",
    "                 'LITHO_CODE', 'MAP_SYMBOL', 'DRILLHOLE_NUMBER', \n",
    "                 'DH_DEPTH_FROM', 'DH_DEPTH_TO', 'SITE_NO', 'EASTING_GDA2020',\n",
    "                 'NORTHING_GDA2020', 'ZONE_GDA2020', 'LONGITUDE_GDA2020', \n",
    "                 'LATITUDE_GDA2020', 'LONGITUDE_GDA94', 'LATITUDE_GDA94', \n",
    "                 'SAMPLE_ANALYSIS_NO', 'CHEM_CODE', 'VALUE', 'UNIT', \n",
    "                 'CHEM_METHOD_CODE']\n",
    "\n",
    "# define an empty dataframe to contain the data related\n",
    "interested_rs_chem_exp = pd.DataFrame(columns = selected_chem_exp_cols)\n",
    "interested_rs_chem_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:25:24.109799Z",
     "start_time": "2020-05-02T06:25:24.104781Z"
    }
   },
   "outputs": [],
   "source": [
    "interested_chemicals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the file 'sarig_rs_chem_exp.csv' is a large file, it cannot be loaded into the memory for most personal computer unless the computer has a RAM more than 16GB.  Here, we read chunk by chunk and map the functions to chunks specifically so as to reduce the times to read this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:24.171992Z",
     "start_time": "2020-05-02T06:25:24.976639Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the sarig_rs_chem_exp by chunks to extract the set of CHEM_CODE\n",
    "sarig_rs_chem_exp_chunks = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name), 'r').open('SARIG_Data_Package/sarig_rs_chem_exp.csv', 'r'), \n",
    "    sep=',', encoding='latin1', chunksize=100000)\n",
    "\n",
    "for chunk in sarig_rs_chem_exp_chunks:    \n",
    "#     try:\n",
    "    chunk_selected = chunk[chunk['CHEM_CODE'].isin(interested_chemicals)][selected_chem_exp_cols]\n",
    "#    chunk_selected['COLLECTED_DATE'] = pd.to_datetime(chunk_selected['COLLECTED_DATE'])\n",
    "    chunk_selected['VALUE'] = chunk_selected['VALUE'].apply(lambda x: change_type(x))\n",
    "    chunk_selected['VALUE'] = chunk_selected['VALUE'].astype('float32')\n",
    "    chunk_selected.loc[:, ['VALUE', 'UNIT']] = chunk_selected.loc[:, ['VALUE', 'UNIT']].apply(lambda x: change_units(x), axis=1)\n",
    "    interested_rs_chem_exp = interested_rs_chem_exp.append(chunk_selected)\n",
    "#     except:\n",
    "#         print('one chunk does not contain the chemicals.\\n')\n",
    "#         pass\n",
    "\n",
    "interested_rs_chem_exp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Export the SITE_NO and SAMPLE_NO for the convenience to select the data from other group of datasets.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:27.000453Z",
     "start_time": "2020-05-02T06:34:24.173960Z"
    }
   },
   "outputs": [],
   "source": [
    "selected_rs_chem_site_sample_num = interested_rs_chem_exp[['SITE_NO', 'SAMPLE_NO']].drop_duplicates()\n",
    "element_path = './data/{}'.format(element_selected)\n",
    "os.mkdir(element_path) \n",
    "selected_rs_chem_site_sample_num.to_csv('./data/{}/rs_chem_site_sample_num.csv'.format(element_selected), sep=',', header='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can obtain the list of SITE_NO and SAMPLE_NO corresponding to the records of selected CHEM_CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load other data table and obtain the records corresponding to the CHEM_CODE selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:27.285964Z",
     "start_time": "2020-05-02T06:34:27.002634Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_biostr_analys_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_biostr_analys_exp.csv','r'), \n",
    "    sep=',', encoding='latin1')\n",
    "sarig_rs_biostr_analys_exp['DRILLHOLE_NO'] = sarig_rs_biostr_analys_exp['DRILLHOLE_NO'].astype('Int64')\n",
    "sarig_rs_biostr_analys_exp['ANALYSIS_DATE'] = pd.to_datetime(sarig_rs_biostr_analys_exp['ANALYSIS_DATE']) \n",
    "sarig_rs_biostr_analys_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:27.313864Z",
     "start_time": "2020-05-02T06:34:27.287963Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_biostr_analys_exp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table contains many nulls and these columns do not contain much useful information for our prediction, so discard some columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:27.337798Z",
     "start_time": "2020-05-02T06:34:27.315860Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "rs_biostr_analys_exp_cols = ['SAMPLE_NO', 'SAMPLE_ANALYSIS_NO', 'OTHER_SAMPLE_NO', 'DRILLHOLE_NO',\n",
    "       'DRILLHOLE_NAME', 'DEPTH_FROM', 'DEPTH_TO', 'SAMPLE_SOURCE_CODE',\n",
    "       'MAP_SYMBOL', 'LITHOLOGY_CODE', 'YOUNGEST_AGE',  'OLDEST_AGE', 'PALEO_ENVIRONMENT', \n",
    "       'SITE_NO', 'SVY_METHOD_CODE']\n",
    "\n",
    "interested_rs_biostr_analys_exp = sarig_rs_biostr_analys_exp[rs_biostr_analys_exp_cols]\n",
    "interested_rs_biostr_analys_exp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Merge with the biostratigraphic analysis data corresponding to the selected CHEM_CODE.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:31.438864Z",
     "start_time": "2020-05-02T06:34:27.339793Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data = interested_rs_chem_exp.merge(\n",
    "    interested_rs_biostr_analys_exp, how='left', on=['SITE_NO', 'SAMPLE_NO'],\n",
    "    suffixes=('_chem', '_biostr'))\n",
    "del interested_rs_biostr_analys_exp\n",
    "extracted_rs_data.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load biostratigraphic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:31.683186Z",
     "start_time": "2020-05-02T06:34:31.441829Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_biostr_results_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_biostr_results_exp.csv','r'), \n",
    "    sep=',', encoding='latin1')\n",
    "sarig_rs_biostr_results_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:31.729067Z",
     "start_time": "2020-05-02T06:34:31.688171Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_biostr_results_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:31.745019Z",
     "start_time": "2020-05-02T06:34:31.735044Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_biostr_results_exp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:36.784547Z",
     "start_time": "2020-05-02T06:34:31.752997Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data = extracted_rs_data.merge(\n",
    "    sarig_rs_biostr_results_exp[['SAMPLE_NO', 'SAMPLE_ANALYSIS_NO', \n",
    "                                 'ANALYSIS_TYPE', 'BIOSTRAT_RESULT',\n",
    "                                 'BIOSTRAT_RESULT_TYPE', 'TYPE_SPECIMEN', \n",
    "                                 'FIGURED_SPECIMEN', 'COMMENTS', 'SITE_NO']], \n",
    "    how='left', on=['SITE_NO', 'SAMPLE_NO'], suffixes=('', '_result'))\n",
    "extracted_rs_data['DRILLHOLE_NO'] = extracted_rs_data['DRILLHOLE_NO'].astype('Int64')\n",
    "del sarig_rs_biostr_results_exp\n",
    "extracted_rs_data.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:36.983017Z",
     "start_time": "2020-05-02T06:34:36.787539Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data['UNIT'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Chemical isotope data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:37.150569Z",
     "start_time": "2020-05-02T06:34:36.985011Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_chem_isotope_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_chem_isotope_exp.csv','r'), \n",
    "    sep=',', encoding='utf-8')\n",
    "sarig_rs_chem_isotope_exp['ANALYSIS_DATE'] = pd.to_datetime(sarig_rs_chem_isotope_exp['ANALYSIS_DATE'])\n",
    "sarig_rs_chem_isotope_exp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the 'UNIT' is not unique, so the 'VALUE' should be adjusted when we unified the UNIT. Further study is needed to unify the units if possible, otherwise, these two columns will be split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:37.172510Z",
     "start_time": "2020-05-02T06:34:37.152563Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_chem_isotope_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:37.209412Z",
     "start_time": "2020-05-02T06:34:37.176505Z"
    }
   },
   "outputs": [],
   "source": [
    "rs_chem_isotope_exp_cols = [\n",
    "    'SAMPLE_NO', 'SAMPLE_ANALYSIS_NO', 'OTHER_ANALYSIS_ID', 'ANALYSIS_TYPE',\n",
    "    'SPECIMEN_TYPE', 'MIN_SEPARATE_MINERAL', 'STRATIGRAPHIC_UNIT', 'MAP_SYMBOL',\n",
    "    'ROCK_GROUP', 'LITHOLOGY', 'ANALYSIS_POINT_NO', 'ANALYSIS_POINT_ID',\n",
    "    'POINT_MINERAL', 'ANALYTE', 'VALUE', 'UNIT', 'UNCERTAINTY_VALUE',\n",
    "    'UNCERTAINTY_UNIT', 'ANALYSIS_METHOD_CODE', 'SITE_NO']\n",
    "interested_rs_chem_isotope_exp = sarig_rs_chem_isotope_exp[rs_chem_isotope_exp_cols]\n",
    "interested_rs_chem_isotope_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:34:43.601326Z",
     "start_time": "2020-05-02T06:34:37.212404Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data = extracted_rs_data.merge(\n",
    "    interested_rs_chem_isotope_exp, how='left', on=['SITE_NO', 'SAMPLE_NO'], \n",
    "    suffixes=('', '_isotope'))\n",
    "del interested_rs_chem_isotope_exp\n",
    "extracted_rs_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the rs_details data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:04.424336Z",
     "start_time": "2020-05-02T06:34:43.603321Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_details_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_details_exp.csv','r'), \n",
    "    sep=',', encoding='latin1')\n",
    "sarig_rs_details_exp['COLLECTED_DATE'] = pd.to_datetime(sarig_rs_details_exp['COLLECTED_DATE'])\n",
    "sarig_rs_details_exp['DRILLHOLE_NUMBER'] = sarig_rs_details_exp['DRILLHOLE_NUMBER'].astype('Int64')\n",
    "sarig_rs_details_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:04.438299Z",
     "start_time": "2020-05-02T06:35:04.426359Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_details_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:04.981847Z",
     "start_time": "2020-05-02T06:35:04.440296Z"
    }
   },
   "outputs": [],
   "source": [
    "rs_details_exp_cols = ['SAMPLE_NO', 'SAMPLE_SOURCE_CODE', 'ROCK_GROUP_CODE',\n",
    "       'ROCK_GROUP', 'LITHO_CODE', 'LITHO_MODIFIER', 'MAP_SYMBOL', \n",
    "       'PARENT_SAMPLE_NO', 'RS_NUMBER', 'DRILLHOLE_NUMBER', 'DH_UNIT_NO', \n",
    "        'DH_DEPTH_FROM', 'DH_DEPTH_TO', 'GEOCHEMISTRY', 'PETROLOGY', \n",
    "        'BIOSTRATIGRAPHY', 'IMAGE', 'MAP_250000', 'MAP_100000', 'MAP_50000',\n",
    "       'SITE_NO', 'SURVEY_METHOD_CODE']\n",
    "interested_rs_details_exp = sarig_rs_details_exp[rs_details_exp_cols]\n",
    "interested_rs_details_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:21.435293Z",
     "start_time": "2020-05-02T06:35:04.984839Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data = extracted_rs_data.merge(\n",
    "    interested_rs_details_exp, how='left', on=['SITE_NO', 'SAMPLE_NO'], \n",
    "    suffixes=('', '_details'))\n",
    "del interested_rs_details_exp\n",
    "extracted_rs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the geochronical age data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:21.552938Z",
     "start_time": "2020-05-02T06:35:21.443218Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_geochron_ages_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_geochron_ages_exp.csv','r'), \n",
    "    sep=',', encoding='latin1')\n",
    "sarig_rs_geochron_ages_exp['INTERPRETATION_DATE'] = pd.to_datetime(sarig_rs_geochron_ages_exp['INTERPRETATION_DATE'])\n",
    "sarig_rs_geochron_ages_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:21.577885Z",
     "start_time": "2020-05-02T06:35:21.557915Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_geochron_ages_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:21.626727Z",
     "start_time": "2020-05-02T06:35:21.580849Z"
    }
   },
   "outputs": [],
   "source": [
    "rs_geochron_ages_exp_cols = ['SAMPLE_NO', 'INTERPRETATION_NO', \n",
    "    'INTERPRETATION_GROUP_NO', 'DATING_METHOD', 'AGE', 'AGE_UNIT', \n",
    "    'AGE_ERROR_MIN', 'AGE_ERROR_MAX', 'AGE_ERROR_UNIT', \n",
    "    'GEOLOGICAL_ATTRIBUTION', 'AGE_TYPE', 'MSWD', 'PROBABILITY_OF_FIT',\n",
    "    'N_OF_ANALYSIS', 'STRATIGRAPHIC_UNIT', 'MAP_SYMBOL', 'ROCK_GROUP', \n",
    "    'LITHOLOGY', 'SITE_NO']\n",
    "interested_rs_geochron_ages_exp = sarig_rs_geochron_ages_exp[rs_geochron_ages_exp_cols]\n",
    "interested_rs_geochron_ages_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:40.914701Z",
     "start_time": "2020-05-02T06:35:21.639693Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data = extracted_rs_data.merge(\n",
    "    interested_rs_geochron_ages_exp, how='left', on=['SITE_NO', 'SAMPLE_NO'], \n",
    "    suffixes=('', '_ages'))\n",
    "del interested_rs_geochron_ages_exp\n",
    "extracted_rs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the geochronical results data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:41.014434Z",
     "start_time": "2020-05-02T06:35:40.924646Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_geochron_reslt_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_geochron_reslt_exp.csv','r'), \n",
    "    sep=',', encoding='latin1')\n",
    "sarig_rs_geochron_reslt_exp['ANALYSIS_DATE'] = pd.to_datetime(sarig_rs_geochron_reslt_exp['ANALYSIS_DATE'])\n",
    "sarig_rs_geochron_reslt_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:41.031389Z",
     "start_time": "2020-05-02T06:35:41.016401Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_geochron_reslt_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:41.060286Z",
     "start_time": "2020-05-02T06:35:41.033358Z"
    }
   },
   "outputs": [],
   "source": [
    "rs_geochron_reslt_exp_cols = ['INTERPRETATION_NO', 'INTERPRETATION_GROUP_NO', 'SAMPLE_NO',\n",
    "       'SAMPLE_ANALYSIS_NO', 'ANALYSIS_TYPE', 'SPECIMEN_TYPE',\n",
    "       'MIN_SEPARATE_MINERAL', 'STRATIGRAPHIC_UNIT', 'MAP_SYMBOL',\n",
    "       'ROCK_GROUP', 'LITHOLOGY', 'ANALYSIS_POINT_NO', 'ANALYSIS_POINT_ID',\n",
    "       'POINT_MINERAL', 'ANALYTE', 'VALUE', 'UNIT', 'UNCERTAINTY_VALUE',\n",
    "       'UNCERTAINTY_UNIT', 'ANALYSIS_METHOD_CODE', 'SITE_NO']\n",
    "interested_rs_geochron_reslt_exp = sarig_rs_geochron_reslt_exp[rs_geochron_reslt_exp_cols]\n",
    "interested_rs_geochron_reslt_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:35:41.120152Z",
     "start_time": "2020-05-02T06:35:41.062280Z"
    }
   },
   "outputs": [],
   "source": [
    "unit_pivot_geochron_reslt_exp = pd.pivot_table(sarig_rs_geochron_reslt_exp[['SITE_NO', 'SAMPLE_NO', 'VALUE', 'UNIT'] ], \n",
    "               values='VALUE', index=['SITE_NO', 'SAMPLE_NO'], columns=['UNIT']).reset_index()\n",
    "interested_rs_geochron_reslt_exp = interested_rs_geochron_reslt_exp.drop(['VALUE', 'UNIT'], axis=1).merge(\n",
    "    unit_pivot_geochron_reslt_exp, \n",
    "    how='inner',\n",
    "    on=['SITE_NO', 'SAMPLE_NO'])\n",
    "del sarig_rs_geochron_reslt_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:36:20.887969Z",
     "start_time": "2020-05-02T06:35:41.122118Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data = extracted_rs_data.merge(\n",
    "    interested_rs_geochron_reslt_exp, how='left', on=['SITE_NO', 'SAMPLE_NO'], \n",
    "    suffixes=('', '_res'))\n",
    "del interested_rs_geochron_reslt_exp\n",
    "extracted_rs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the petrology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:36:21.638966Z",
     "start_time": "2020-05-02T06:36:20.893952Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_petrology_exp = pd.read_csv(\n",
    "    zipfile(os.path.join(data_loc, file_name),'r').open('SARIG_Data_Package/sarig_rs_petrology_exp.csv','r'), \n",
    "    sep=',', encoding='latin1')\n",
    "\n",
    "sarig_rs_petrology_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:36:21.681851Z",
     "start_time": "2020-05-02T06:36:21.640961Z"
    }
   },
   "outputs": [],
   "source": [
    "sarig_rs_petrology_exp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:36:21.706787Z",
     "start_time": "2020-05-02T06:36:21.683846Z"
    }
   },
   "outputs": [],
   "source": [
    "rs_petrology_exp_cols = ['SAMPLE_NO', 'SAMPLE_ANALYSIS_NO', 'THIN_SECTION_NO',\n",
    "       'HISTORICAL_PETROLOGY_NO', 'ROCK_TYPE_CODE', 'MINERALS_ABUNDANT',\n",
    "       'MINERALS_MAJOR_ABUNDANCE', 'MINERALS_MINOR_ABUNDANCE',\n",
    "       'MINERALS_TRACE_ABUNDANCE', 'MINERALS_RARE_ABUNDANCE',\n",
    "       'MINERALS_UNKNOWN_ABUNDANCE', 'SITE_NO']\n",
    "interested_rs_petrology_exp = sarig_rs_petrology_exp[rs_petrology_exp_cols]\n",
    "interested_rs_petrology_exp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:37:26.947110Z",
     "start_time": "2020-05-02T06:36:21.708781Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data = extracted_rs_data.merge(\n",
    "    interested_rs_petrology_exp, how='left', on=['SITE_NO', 'SAMPLE_NO'], \n",
    "    suffixes=('', '_petro'))\n",
    "del interested_rs_petrology_exp\n",
    "extracted_rs_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the merged rs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T06:39:05.504101Z",
     "start_time": "2020-05-02T06:37:26.974040Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_rs_data.to_csv('./data/{}/extracted_rs_data.csv'.format(element_selected), sep=',', header='infer')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
